{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1e9bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jigyanshupati/nanogpt/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/shubhammaindola/harry-potter-books?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.28M/2.28M [00:01<00:00, 1.29MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/jigyanshupati/.cache/kagglehub/datasets/shubhammaindola/harry-potter-books/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"shubhammaindola/harry-potter-books\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57e4040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439478"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/jigyanshupati/nanogpt/01 Harry Potter and the Sorcerers Stone.txt\") as file:\n",
    "    books = file.read()\n",
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da1f17ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 82\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(books)))\n",
    "vocab_size = len(characters)\n",
    "print(\"Number of unique characters:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19880e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create mapping from int to char and char to int\n",
    "int_to_char = {i: c for i, c in enumerate(characters)}\n",
    "char_to_int = {c: i for i, c in enumerate(characters)}\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "#function to convert text to integers\n",
    "def encode(text):\n",
    "    return enc.encode(text)\n",
    "#function to convert integers to text\n",
    "def decode(ints):\n",
    "    return enc.decode(ints)\n",
    "\n",
    "decode(encode(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "33fc6a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.11/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([104922])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "import torch\n",
    "data= torch.tensor(encode(books), dtype=torch.long)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4c7ed2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f3067a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   44,   428,    13,   326, 31127,    13,   415,  1628,  4356])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e0d6d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "torch.manual_seed(42)\n",
    "def get_batch(split):\n",
    "    #generate random batch of data\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6a71a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!WQ0[9K>)M;TL)MBa*9#f0f$iaoP;ff!F@5@!dWS<@P5NEXP^b9^/NF@eo=Aq$,B-Qmc-f%1dEof;,1b;\"#@<GKm3+TJn:R5:6nLJ#r#gM]#l]MoC@SI=q$*qfoj=6f*])1_e@<E;5!JPI,iRL@Q<]oY@(*#>Hf@hfGfo9g>QJ1KBR]J=EQ#@8eWbq1i7]eD3D<]mZQ#=DB3P]]3@[+P-<RQ\\\\EV\"i#?qgBi:;Agcf58!`!B!?QX;<@?\"?BTl9_-BY-@D\\\\f0_G.I]qP%-6<,4T!=qrMB++o?_gq&X]!347<eDh<-/TEq$;ZfoVE0Rbn<1];OB;.]Q!Q%JE.mqAf=q**#]#fH;$$D\\\\]a5Q8TmZKTk](QGP\"9MBo%]ZXE$NKfSJWA3*-mY>5@<C$D#\"&/5%(\\\\\"3#D3pm)]]3G81,ff+(6!a(\"#]<Ba\\'B3ao@;`d%P`=L@\\'D%,qMGKce]n*;/K5%0Q9m:MQ3GB0q1Gq.V5X8a.Q03jG>4fBSBQeW<@(#ef_,2To-iN?Q!>,!!Ki6.l]<I\\\\,8<&g5.@r.G*?9.b&i#lpQqG-EPQ]_/Oq!\"?]K>YrOQNgZ<,B\\\\Zq-h\\\\@h\"PBA7A\"Q\"f9CWea-//rr513/)fSc6F5]3o%pq3_A[1DK+p>d%Ba8qrA5i5\"=DT*$j!S>@N<q@<p@c,!!?_8)qD61@N3C^>3F<Neb.:\"Q#!7oQ3n1O+`MP2\\\\]P<1H1ZX](qI68@<@<1]]1@p(0V<@<9K>SBMR?od3#fOGMBf%JpLTL(I$#@BF-q$nr?QZ1:451a*?%]Ma;fm!#W:5eS*i29n*oT%Te^ED!!abOo\\\\P70N.m;f<!@%.4LY1P3C)?QVG1Z8.ZUJnWC-?q$*/2&fSGYR\\\\]_qLF<T5+Heo!/H]FT5!E/ca.B%_9BibS*.)]Po%*S?QFo(kaGS5[.0N`87\\\\%plm?$*Tnq;dB@DBfTN.,Y:=;h?<1].eO0.R\\\\0F_Bfd%=JR5M>_1(/)8?J<oGiB<@V@\"MS'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, vocab_size)\n",
    "        self.lm_head = nn.Linear(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,C)\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # (B*T,C)\n",
    "            targets = targets.view(B*T) # (B*T,)\n",
    "            loss = F.cross_entropy(logits, targets) # (B*T,)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  # (B, T)\n",
    "            logits = self(idx_cond)           # get only logits here\n",
    "            logits = logits[:, -1, :]         # focus on last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "m=BigramLanguageModel(vocab_size,block_size)\n",
    "# logits,loss=m(xb, yb)  # -2.1813e+00\n",
    "decode(m.generate(torch.zeros((1, 1), dtype=torch.long), 1000)[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1a8f2d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss 2.4456\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[177]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m xb, yb = get_batch(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m logits, loss = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nanogpt/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nanogpt/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[176]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mBigramLanguageModel.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     13\u001b[39m     B, T = idx.shape\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     tok_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[32m     15\u001b[39m     pos_emb = \u001b[38;5;28mself\u001b[39m.position_embedding_table(torch.arange(T, device=idx.device)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[32m     16\u001b[39m     x = tok_emb + pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nanogpt/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nanogpt/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nanogpt/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nanogpt/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "#training loop\n",
    "for i in range(100000):\n",
    "    print(f\"step {i}: loss {loss.item():.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "# plotting loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6dfaae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". “If I’t domend brevey. — Hak y se f hon’thrn in — s pldno bere, is?” toresple s, tas.  hevid mbare ve e hi\n"
     ]
    }
   ],
   "source": [
    "logits.shape\n",
    "print(decode(m.generate(xb, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.0418, -0.2516,  0.8599],\n",
      "         [-0.7538, -0.4219, -0.0092,  ...,  1.5011, -1.0068,  1.2838],\n",
      "         [ 1.4646,  0.9943,  1.5864,  ..., -0.3506,  0.5980,  0.6755],\n",
      "         ...,\n",
      "         [-0.4599,  0.2508, -0.0482,  ...,  0.4776, -0.3928,  0.8727],\n",
      "         [ 1.5130,  0.9772,  1.4039,  ..., -0.4304,  0.5649,  0.6918],\n",
      "         [-0.4334,  0.0063,  0.4008,  ..., -0.2636,  0.6562,  0.9794]],\n",
      "\n",
      "        [[-0.6855,  0.5636, -1.5072,  ...,  1.1566,  0.2691, -0.0366],\n",
      "         [ 0.0175, -0.1055, -1.0981,  ...,  0.6430,  0.6657, -0.4375],\n",
      "         [ 0.0825, -0.3012,  0.1249,  ..., -0.0343, -0.2112, -0.2610],\n",
      "         ...,\n",
      "         [ 0.7233, -0.8095, -0.4075,  ...,  0.0813,  0.8639, -0.7859],\n",
      "         [-0.4920,  0.2737, -1.0159,  ...,  0.9975,  0.4399, -0.2065],\n",
      "         [ 0.7317, -0.7136, -0.4925,  ...,  0.1837,  0.9935, -0.8176]],\n",
      "\n",
      "        [[-1.3638,  0.1930, -0.6103,  ...,  0.6110,  1.2208, -0.6076],\n",
      "         [-1.7060, -0.0985, -1.3020,  ..., -0.5010, -0.0565,  0.5065],\n",
      "         [-1.1378, -0.7251, -0.3563,  ..., -0.0636,  1.1076,  0.5533],\n",
      "         ...,\n",
      "         [ 2.0014, -1.4773,  0.1426,  ...,  2.9176,  1.3302,  0.8212],\n",
      "         [-0.3552,  0.1917,  0.9943,  ..., -0.3539, -0.4259, -0.4729],\n",
      "         [-0.9741,  0.3572,  1.3926,  ...,  0.3590, -0.2362, -0.9819]],\n",
      "\n",
      "        [[ 0.3444, -3.1016, -1.4587,  ...,  1.4162,  0.6834, -0.1383],\n",
      "         [ 0.6145, -1.4017, -0.7795,  ...,  1.7588,  0.0541, -0.0203],\n",
      "         [ 0.1271,  0.4507,  0.1453,  ...,  1.4335, -0.3127,  1.1391],\n",
      "         ...,\n",
      "         [-0.1617, -1.5053, -1.1589,  ...,  0.7412, -0.4846, -0.4208],\n",
      "         [ 0.2719, -2.5947, -1.4379,  ...,  1.3675,  0.4532, -0.1923],\n",
      "         [ 0.1329, -0.3500, -0.2275,  ...,  1.3863, -0.0208,  0.2554]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#self attention\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,32\n",
    "x1=torch.randn(B,T,C) # (B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key=nn.Linear(C, head_size, bias=False) # (C,head_size)\n",
    "query=nn.Linear(C, head_size, bias=False) # (C,head_size)\n",
    "value=nn.Linear(C, head_size, bias=False) # (C,head_size)\n",
    "k=key(x1) # (B,T,head_size)\n",
    "q=query(x1) # (B,T,head_size)\n",
    "weights = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "tril= torch.tril(torch.ones((T, T))) # (T,T)\n",
    "# weights = torch.zeros((T, T)) # (T,T)\n",
    "weights=weights.masked_fill(tril==0, float('-inf')) # (T,T)\n",
    "weights=F.softmax(weights, dim=-1) # (T,T)\n",
    "output = torch.matmul(weights, x1) # (B,T,C)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1a3b9528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "46b34775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [tiktoken]\n",
      "\u001b[1A\u001b[2KSuccessfully installed regex-2024.11.6 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ad1b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8e058105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(enc.encode(\"hello world\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "812e55d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2851, -0.2685, -0.3118,  1.0101,  1.9954],\n",
      "        [-0.9788, -0.6320,  0.1928,  1.2019,  0.8037],\n",
      "        [-1.2548, -0.4295,  0.9173, -0.7931, -0.2413],\n",
      "        [ 0.2915,  0.4070, -0.1934,  1.1370,  0.1862],\n",
      "        [-0.1693, -0.3676,  0.0641, -1.6502,  0.6944]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for MPS support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Create a tensor and move to MPS\n",
    "x = torch.randn(5, 5).to(device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f95483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
